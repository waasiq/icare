<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="./docs/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet">
    <title>Document</title>
</head>
<body>
    
    <nav class="navigation-bar" id="navbar">
        <span class="navigation-toggle" id="navbar-toggle">
            <span></span>
            <span></span>
            <span></span>
        </span>
         <div class="navigation-nav" id="navbar-nav">
             <a href="#intro" class="navigation-nav-link" target="_self">Intro</a>
             <a href="https://opencv.org/" class="navigation-nav-link" target="_blank">OpenCV</a>
             <a href="https://google.github.io/mediapipe/" class="navigation-nav-link" target="_blank">Mediapipe</a>
             <a href="./docs/about.html" class="navigation-nav-link" target="_blank">About</a>
         </div>
    </nav>

    <section class="intro" id="intro">
        <h1>
            iCare
        </h1> 
        <p>
            iCare is a python application which makes use of your webcam in order to detect the 
            emotion on your face. iCare uses modules from <a href="https://google.github.io/mediapipe/">MediaPipe</a> 
            and <a href="https://opencv.org/">OpenCV</a>. Detection of emotions is done mathematically by calculating 
            difference between various specific points for each emotion.

            iCare is quite in it's intial development and is a unique idea by two university students in Turkey - Waasiq Masood
            and Eyup Barlas. The module will contain a lot of improvements in upcoming versions. More is explained in the working
            of the program.
        </p>

        <img src="./docs/logo/facebook_cover_photo_1.png" alt="cover">
    </section>

    <section class="contents">
        <p>Table of contents</p>
        <ul>
            <li><a href="#working" target="_self">Working</a></li>
            <li><a href="#overview" target="_self">Modules overview and Code Structure</a></li>
            <li><a href="#opencv" target="_self">OpenCV and MediaPipe Facemesh</a></li>
            <li><a href="#landmarks" target="_self">Facemesh Landmarks</a></li>
            <li><a href="#emotions" target="_self">Detecting of emotions</a></li>
            <li><a href="#future" target="_self">Future Plans (Selenium)</a></li>
        </ul>
    </section>

    <section class="working" id="working">
        <h1>Working</h1>
        <p><span>iCare combines the modules of OpenCV and MediPipe together. The working of camera 
        is controlled by OpenCV while the detection of facial landmarks is done using MediaPipe.
        We are capturing the specific points of the facemesh and then calculating the change between those
        points in order to understand which emotion is being shown on the screen. 
        
        Ofcourse the capture is not point perfect the main reason it being is we were testing on laptop webcams. 
        To capture the micro emotions of the human facial expression, high HD cameras are required. The other
        problem which we encountered was that while happiness may be seen on face easily sadness and anger has 
        microexpressions on the face which is quite hard to calculate mathematically. 
        
        <div class="micro-expression">
            <img src="./docs/img/micro-expression-happiness.jpg" alt="happiness">
            <img src="./docs/img/micro-expression-sadness.jpg" alt="sadness">
        </div>
        </span> </p>
    </section>

    <!-- Modules -->
    <section class="working" id="overview">
        <h1>Modules Overview and Code Structure</h1>

        <h3>Modules Overview</h3>
        <p><span>We have coded three main modules which are then used in the program. FaceDetectionModule.py detects
        the face of a human and gives a success percentage. This module was implemented in the older versions of our program
        but was replaced with more complex Facemesh module. Facemesh module is explained in the facemesh module.    
        </span></p>

        <h3>Code Structure</h3>
        <p><span>Although all the emotions have been combined in iCareProducts/emotions.py. The emotion file uses the 
        landmark detection of the respective emotion file. It imports the function 'emotion'Points function from 
        the respective emotion file which returns us a left and right hypotenus value. The data is evaluated in realtime 
        and the emotion is calculated then. FacemeshDetection function from Facemesh module is also used in the emotions module.</span> </p>

        <img class="structure" src="./docs/img/structure.png" alt="structure">
    </section>

    <section class="working" id="opencv">
        <h1>OpenCV and Facemesh Module</h1>
        <p><span>OpenCV is being used to control the video part of the application. We use the Facemesh to capture the face of the person.
        More information on OpenCV and Facemesh can be found in the documentations of OpenCV and Mediapipe.
        </span></p>      
    </section>

    <section class="working" id="landmarks">
        <h1>Facemesh Landmarks</h1>
        <p><span>Facemesh contains about 450+ landmarks on the face. Each of the landmark can be represented a digit value. Below is the image and link 
            attached to get the specific landmarks of the face. For every emotion we have captured two different landmarks on each side of face which is explained
            in the capturing emotions part.

            <div class="micro-expression">
                <img src="./docs/img/facemesh.png" alt="facemesh">
                <img src="./docs/img/landmarks.png" alt="landmarks">
            </div>
        </span></p>      
    </section>

    <section class="working" id="emotions">
        <h1>Detecting of emotions</h1>
        <p><span>
            Detecting emotions has been done by calculating the changes in hypotenus when the face is in a specific position.
            The specific box has been put in order to calculate the differences from a exact point else capturing the Z axis 
            is quite a hard thing. Below is the given example of a person inside a box and outside the box: 

            <div class="micro-expression">
                <img src="./docs/img/square.png" alt="square">
                <img src="./docs/img/not-inside.png" alt="landmarks">
            </div>

            To calculate the smile we are calculating the left and right hypotenus in the program. In each of the emotion there 
            is hypotenus calculation in the detection function. For example: the hypotenus for left and right is calculated in 
            the smilingPoints function by subtracting the top and bottom X and Y respectively.
            
            </br>   
            </br>
            <span><i>hypotenus = math.hypot(topX - botX, topY - topX ) </i></span>

            <div class="micro-expression">
                <img src="./docs/img/smile.png" alt="square">
                <img src="./docs/img/not-smile.png" alt="landmarks">
            </div>

        </span></p>      
    </section>

    <section class="working" id="sources">
        <h1>Useful sources used</h1>
        <p><span>Useful links:
           <a href="https://github.com/ManuelTS/augmentedFaceMeshIndices" target="_blank">Face mesh Landmark</a>
           <a href="https://www.youtube.com/watch?v=B0ouAnmsO1Y&ab_channel=TheOregonian" target="_blank">Facial expressions video</a>
        </span></p>      
    </section>
    <section class="working" id="future">
        <h1>Future plans and selenium module</h1>
        <p><span>404 Under Construction
        </span></p>      
    </section>

   
</body>

<script src="./docs/app.js"></script>
</html>